라이브러리 임포트
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers,models
import numpy as np
import matplotlib.pyplot as plt

데이터로드&전처리
(x_train,y_train),(x_test,y_test) =keras.datasets.cifar10.load_data()
x_train,x_test=x_train.astype('float32')/255.0,x_test.astype('float32')/255.0
num_classes=10
y_train=keras.utils.to_categorical(y_train,num_classes)
y_test=keras.utils.to_categorical(y_test,num_classes)

모델 정의
model=models.Sequential()
model.add(layers.Conv2D(32,(3,3),padding='same',activation='relu',input_shape=(32,32,3)))
model.add(layers.Conv2D(32,(3,3),activation='relu'))
model.add(layers.MaxPooling2D((2,2,)))
model.add(layers.Dropout(0.25))

model.add(layers.Conv2D(64, (3, 3), padding="same", activation="relu"))
model.add(layers.Conv2D(64, (3, 3), activation="relu"))
model.add(layers.MaxPooling2D((2, 2)))
model.add(layers.Dropout(0.25))

model.add(layers.Flatten())
model.add(layers.Dense(512, activation="relu"))
model.add(layers.Dropout(0.5))
model.add(layers.Dense(num_classes, activation="softmax"))

모델 컴파일&학습
model.compile(optimizer="adam",
              loss="categorical_crossentropy",
              metrics=["accuracy"])
history = model.fit(
    x_train, y_train,
    epochs=50,
    batch_size=64,
    validation_split=0.1,
    verbose=2
)

Epoch 1/50
704/704 - 12s - 17ms/step - accuracy: 0.4306 - loss: 1.5567 - val_accuracy: 0.5810 - val_loss: 1.1856
Epoch 2/50
704/704 - 4s - 6ms/step - accuracy: 0.5875 - loss: 1.1531 - val_accuracy: 0.6802 - val_loss: 0.9333
Epoch 3/50
704/704 - 4s - 6ms/step - accuracy: 0.6549 - loss: 0.9756 - val_accuracy: 0.7128 - val_loss: 0.8164
Epoch 4/50
704/704 - 4s - 6ms/step - accuracy: 0.6923 - loss: 0.8739 - val_accuracy: 0.7402 - val_loss: 0.7397
Epoch 5/50
704/704 - 4s - 6ms/step - accuracy: 0.7214 - loss: 0.7911 - val_accuracy: 0.7512 - val_loss: 0.7167
Epoch 6/50
704/704 - 4s - 6ms/step - accuracy: 0.7406 - loss: 0.7368 - val_accuracy: 0.7636 - val_loss: 0.6855
Epoch 7/50
704/704 - 4s - 6ms/step - accuracy: 0.7576 - loss: 0.6897 - val_accuracy: 0.7642 - val_loss: 0.6821
Epoch 8/50
704/704 - 4s - 6ms/step - accuracy: 0.7703 - loss: 0.6572 - val_accuracy: 0.7894 - val_loss: 0.6210
Epoch 9/50
704/704 - 5s - 7ms/step - accuracy: 0.7816 - loss: 0.6211 - val_accuracy: 0.7924 - val_loss: 0.6235
Epoch 10/50
704/704 - 5s - 7ms/step - accuracy: 0.7893 - loss: 0.5927 - val_accuracy: 0.7952 - val_loss: 0.6041
Epoch 11/50
704/704 - 6s - 8ms/step - accuracy: 0.8016 - loss: 0.5613 - val_accuracy: 0.7806 - val_loss: 0.6658
Epoch 12/50
704/704 - 5s - 7ms/step - accuracy: 0.8083 - loss: 0.5423 - val_accuracy: 0.7966 - val_loss: 0.6138
Epoch 13/50
704/704 - 5s - 8ms/step - accuracy: 0.8160 - loss: 0.5199 - val_accuracy: 0.7966 - val_loss: 0.6146
Epoch 14/50
704/704 - 4s - 5ms/step - accuracy: 0.8226 - loss: 0.4995 - val_accuracy: 0.7970 - val_loss: 0.6090
Epoch 15/50
704/704 - 5s - 7ms/step - accuracy: 0.8277 - loss: 0.4847 - val_accuracy: 0.7996 - val_loss: 0.6180
Epoch 16/50
704/704 - 4s - 6ms/step - accuracy: 0.8365 - loss: 0.4598 - val_accuracy: 0.7896 - val_loss: 0.6466
Epoch 17/50
704/704 - 4s - 6ms/step - accuracy: 0.8382 - loss: 0.4531 - val_accuracy: 0.8002 - val_loss: 0.6319
Epoch 18/50
704/704 - 4s - 6ms/step - accuracy: 0.8413 - loss: 0.4427 - val_accuracy: 0.7984 - val_loss: 0.6361
Epoch 19/50
704/704 - 5s - 7ms/step - accuracy: 0.8477 - loss: 0.4265 - val_accuracy: 0.8062 - val_loss: 0.6268
Epoch 20/50
704/704 - 5s - 7ms/step - accuracy: 0.8524 - loss: 0.4164 - val_accuracy: 0.8042 - val_loss: 0.6052
Epoch 21/50
704/704 - 5s - 8ms/step - accuracy: 0.8580 - loss: 0.4016 - val_accuracy: 0.8050 - val_loss: 0.6401
Epoch 22/50
704/704 - 4s - 6ms/step - accuracy: 0.8603 - loss: 0.3913 - val_accuracy: 0.7908 - val_loss: 0.6444
Epoch 23/50
704/704 - 4s - 6ms/step - accuracy: 0.8651 - loss: 0.3801 - val_accuracy: 0.8088 - val_loss: 0.6194
Epoch 24/50
704/704 - 4s - 6ms/step - accuracy: 0.8640 - loss: 0.3808 - val_accuracy: 0.8018 - val_loss: 0.6354
Epoch 25/50
704/704 - 5s - 7ms/step - accuracy: 0.8668 - loss: 0.3757 - val_accuracy: 0.8040 - val_loss: 0.6260
Epoch 26/50
704/704 - 4s - 6ms/step - accuracy: 0.8723 - loss: 0.3606 - val_accuracy: 0.7980 - val_loss: 0.6690
Epoch 27/50
704/704 - 4s - 6ms/step - accuracy: 0.8728 - loss: 0.3567 - val_accuracy: 0.8082 - val_loss: 0.6273
Epoch 28/50
704/704 - 4s - 5ms/step - accuracy: 0.8749 - loss: 0.3533 - val_accuracy: 0.8032 - val_loss: 0.6360
Epoch 29/50
704/704 - 4s - 6ms/step - accuracy: 0.8779 - loss: 0.3474 - val_accuracy: 0.8034 - val_loss: 0.6398
Epoch 30/50
704/704 - 4s - 6ms/step - accuracy: 0.8804 - loss: 0.3345 - val_accuracy: 0.8108 - val_loss: 0.6274
Epoch 31/50
704/704 - 4s - 6ms/step - accuracy: 0.8823 - loss: 0.3383 - val_accuracy: 0.8104 - val_loss: 0.6560
Epoch 32/50
704/704 - 5s - 7ms/step - accuracy: 0.8811 - loss: 0.3325 - val_accuracy: 0.8038 - val_loss: 0.6452
Epoch 33/50
704/704 - 4s - 6ms/step - accuracy: 0.8870 - loss: 0.3192 - val_accuracy: 0.8076 - val_loss: 0.6537
Epoch 34/50
704/704 - 5s - 7ms/step - accuracy: 0.8861 - loss: 0.3230 - val_accuracy: 0.8110 - val_loss: 0.6352
Epoch 35/50
704/704 - 5s - 7ms/step - accuracy: 0.8880 - loss: 0.3172 - val_accuracy: 0.8104 - val_loss: 0.6378
Epoch 36/50
704/704 - 5s - 7ms/step - accuracy: 0.8908 - loss: 0.3116 - val_accuracy: 0.8092 - val_loss: 0.6771
Epoch 37/50
704/704 - 5s - 7ms/step - accuracy: 0.8913 - loss: 0.3083 - val_accuracy: 0.8084 - val_loss: 0.6377
Epoch 38/50
704/704 - 4s - 5ms/step - accuracy: 0.8928 - loss: 0.3021 - val_accuracy: 0.8068 - val_loss: 0.6500
Epoch 39/50
704/704 - 5s - 8ms/step - accuracy: 0.8929 - loss: 0.3049 - val_accuracy: 0.8112 - val_loss: 0.6275
Epoch 40/50
704/704 - 5s - 7ms/step - accuracy: 0.8944 - loss: 0.2979 - val_accuracy: 0.8078 - val_loss: 0.6687
Epoch 41/50
704/704 - 5s - 7ms/step - accuracy: 0.8981 - loss: 0.2932 - val_accuracy: 0.8032 - val_loss: 0.7254
Epoch 42/50
704/704 - 5s - 7ms/step - accuracy: 0.8958 - loss: 0.2971 - val_accuracy: 0.8028 - val_loss: 0.7016
Epoch 43/50
704/704 - 4s - 6ms/step - accuracy: 0.8994 - loss: 0.2863 - val_accuracy: 0.8138 - val_loss: 0.6571
Epoch 44/50
704/704 - 5s - 8ms/step - accuracy: 0.8979 - loss: 0.2904 - val_accuracy: 0.8118 - val_loss: 0.6517
Epoch 45/50
704/704 - 5s - 7ms/step - accuracy: 0.8999 - loss: 0.2852 - val_accuracy: 0.8124 - val_loss: 0.6490
Epoch 46/50
704/704 - 4s - 6ms/step - accuracy: 0.9016 - loss: 0.2829 - val_accuracy: 0.8108 - val_loss: 0.6791
Epoch 47/50
704/704 - 5s - 7ms/step - accuracy: 0.9010 - loss: 0.2796 - val_accuracy: 0.8110 - val_loss: 0.6668
Epoch 48/50
704/704 - 4s - 6ms/step - accuracy: 0.9056 - loss: 0.2695 - val_accuracy: 0.8104 - val_loss: 0.7045
Epoch 49/50
704/704 - 5s - 8ms/step - accuracy: 0.9027 - loss: 0.2798 - val_accuracy: 0.8066 - val_loss: 0.6931
Epoch 50/50
704/704 - 5s - 7ms/step - accuracy: 0.9052 - loss: 0.2714 - val_accuracy: 0.8082 - val_loss: 0.6877

테스트 평가
test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)
print("Test accuracy:", test_acc)

313/313 - 1s - 4ms/step - accuracy: 0.7928 - loss: 0.7077
Test accuracy: 0.7928000092506409

예측 결과
predictions = model.predict(x_test[:5])
print("예측 결과:", np.argmax(predictions, axis=1))
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 278ms/step
예측 결과: [3 8 8 0 6]

정확도&손실 그래프
fig, axs = plt.subplots(1, 2, figsize=(12, 4))

axs[0].plot(history.history['accuracy'], label='Train Accuracy')
axs[0].plot(history.history['val_accuracy'], label='Validation Accuracy')
axs[0].set_title('Model Accuracy')
axs[0].set_ylabel('Accuracy')
axs[0].set_xlabel('Epoch')
axs[0].legend(loc='lower right')

axs[1].plot(history.history['loss'], label='Train Loss')
axs[1].plot(history.history['val_loss'], label='Validation Loss')
axs[1].set_title('Model Loss')
axs[1].set_xlabel('Epoch')
axs[1].set_ylabel('Loss')
axs[1].legend()

plt.tight_layout()
plt.show()

